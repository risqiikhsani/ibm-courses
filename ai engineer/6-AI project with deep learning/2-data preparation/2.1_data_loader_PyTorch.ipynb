{"cells":[{"cell_type":"markdown","id":"57696b4b-ef79-4705-bd9c-1fdbedbe8682","metadata":{},"outputs":[],"source":["<a href=\"http://cocl.us/pytorch_link_top\">\n","    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n","</a> \n"]},{"cell_type":"markdown","id":"66a652a7-bd21-4b14-911f-f39eb679c951","metadata":{},"outputs":[],"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n"]},{"cell_type":"markdown","id":"a1ee1c10-6862-474a-9f91-de55562860c8","metadata":{},"outputs":[],"source":["<h1>Objective</h1><ul><li> How to create a dataset object.</li></ul> \n"]},{"cell_type":"markdown","id":"d8a41aae-847b-4d13-a561-de4a0001b83a","metadata":{},"outputs":[],"source":["<h1>Data Preparation with PyTorch</h1>\n"]},{"cell_type":"markdown","id":"6ecdb884-7598-46b6-acb5-e3911bde1738","metadata":{},"outputs":[],"source":["<p>Crack detection has vital importance for structural health monitoring and inspection. We would like to train a network to detect Cracks, we will denote the images that contain cracks as positive and images with no cracks as negative.  In this lab you are going to have to build a dataset object. There are five questions in this lab, Including some questions that are intermediate steps to help you build the dataset object. You are going to have to remember the output for some  of the questions. </p>\n"]},{"cell_type":"markdown","id":"da45df08-3ba2-48af-9d69-9b8b6f16cb20","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n"]},{"cell_type":"markdown","id":"e83299f6-fab6-468c-88ed-d140911b571b","metadata":{},"outputs":[],"source":["<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","\n","<ul>\n","    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n","    <li><a href=\"#download_data\"> Download data</a></li>\n","    <li><a href=\"#examine_files\">Examine Files</a></li> \n","    <li><a href=\"#Question_1\"><b>Question 1:find number of files</b> </a></li>\n","    <li><a href=\"#assign_labels\">Assign Labels to Images  </a></li>\n","    <li><a href=\"#Question_2\"><b>Question 2 : Assign labels to image </b> </a></li>\n","    <li><a href=\"#split\">Training  and Validation  Split </a></li>\n","    <li><a href=\"#Question_3\"><b>Question 3: Training  and Validation  Split</b> </a></li>\n","<li><a href=\"#data_class\">Create a Dataset Class </a></li>\n","    <li><a href=\"#Question_4\"><b>Question 4:Display  training dataset object</b> </a></li>\n","    <li><a href=\"#Question_5\"><b>Question 5:Display  validation dataset  object</b> </a></li>\n","\n","</ul>\n","<p>Estimated Time Needed: <strong>25 min</strong></p>\n"," </div>\n","<hr>\n"]},{"cell_type":"markdown","id":"44ff5994-c272-4d20-97a2-1ef1c859e96d","metadata":{},"outputs":[],"source":["<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"]},{"cell_type":"markdown","id":"572654b4-587e-4279-b13c-7977255d1b37","metadata":{},"outputs":[],"source":["The following are the libraries we are going to use for this lab:\n"]},{"cell_type":"code","id":"99b83815-414b-4a76-837d-6f07fa76274b","metadata":{},"outputs":[],"source":["from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset\nimport skillsnetwork \n"]},{"cell_type":"markdown","id":"1bd010de-f754-4850-89e9-7836e2f430ca","metadata":{},"outputs":[],"source":["We will use this function in the lab to plot:\n"]},{"cell_type":"code","id":"53c92cdc-02f7-4fac-8332-0e431670c46c","metadata":{},"outputs":[],"source":["def show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + data_sample[1])"]},{"cell_type":"markdown","id":"496f00b0-cfea-48ab-9b27-f401452ce1c8","metadata":{},"outputs":[],"source":["<h2 id=\"download_data\">Download Data</h2>\n"]},{"cell_type":"markdown","id":"5e9e483c-2674-4911-832f-dd3c5798362a","metadata":{},"outputs":[],"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. <b>skillsnetwork.prepare</b> is a command that's used to download a zip file, unzip it and store it in a specified directory. Locally we store the data in the directory  **/resources/data**. \n"]},{"cell_type":"code","id":"a098e000-7897-4d69-bdf6-a5fe7aff15e8","metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\", path = \"/resources/data\", overwrite=True)"]},{"cell_type":"markdown","id":"91b92ba1-2f89-4541-885c-be5447f9f9bc","metadata":{},"outputs":[],"source":["We then download the files that contain the negative images:\n"]},{"cell_type":"markdown","id":"599b725f-2f11-4680-a3b0-aba669f9a39d","metadata":{},"outputs":[],"source":["<h2 id=\"examine_files\">Examine Files </h2>\n"]},{"cell_type":"markdown","id":"060f7363-a02a-4869-9a00-e8e25736bbc1","metadata":{},"outputs":[],"source":["In the previous lab, we create two lists; one to hold the path to the Negative files and one to hold the path to the Positive files. This process is shown in the following few lines of code.\n"]},{"cell_type":"markdown","id":"c1d06ac4-bc21-4a55-b9e8-ab188d50dcf5","metadata":{},"outputs":[],"source":["We can obtain the list that contains the path to the <b>negative files</b> as follows:\n"]},{"cell_type":"code","id":"1de91b62-1eab-41e3-addf-5ba364c257d6","metadata":{},"outputs":[],"source":["directory=\"/resources/data\"\nnegative='Negative'\nnegative_file_path=os.path.join(directory,negative)\nnegative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\nnegative_files.sort()\nnegative_files[0:3]"]},{"cell_type":"markdown","id":"990c1be7-2292-4ebf-b43d-ba2cb9ff90d7","metadata":{},"outputs":[],"source":["We can obtain the list that contains the path to the <b>positive files</b> files as follows:\n"]},{"cell_type":"code","id":"d615b35a-54fd-4399-8dd8-00109401d4b3","metadata":{},"outputs":[],"source":["positive=\"Positive\"\npositive_file_path=os.path.join(directory,positive)\npositive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\npositive_files.sort()\npositive_files[0:3]"]},{"cell_type":"markdown","id":"50765b89-762e-415c-b4c0-897a8c9560fd","metadata":{},"outputs":[],"source":["<h2 id=\"Question_1\">Question 1</h2>\n","<b>Find the <b>combined</b> length of the list <code>positive_files</code> and <code>negative_files</code> using the function <code>len</code> . Then assign  it to the variable <code>number_of_samples</code></b>\n"]},{"cell_type":"code","id":"b7559bb3-5987-4779-87b8-11c08fcd4024","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"6b7330e3-1e19-4340-b5c4-fbc0504e7f9b","metadata":{},"outputs":[],"source":["<h2 id=\"assign_labels\">Assign Labels to Images </h2>\n"]},{"cell_type":"markdown","id":"76d411ee-320a-44cc-8087-055e55e47c12","metadata":{},"outputs":[],"source":["In this section we will assign a label to each image in this case we  can assign the positive images, i.e images with a crack to a value one  and the negative images i.e images with out a crack to a value of zero <b>Y</b>. First we create a tensor or vector of zeros, each element corresponds to a new sample. The length of the tensor is equal to the number of samples.\n"]},{"cell_type":"code","id":"aec22e38-c541-4842-8df3-ca27d664801d","metadata":{},"outputs":[],"source":["Y=torch.zeros([number_of_samples])"]},{"cell_type":"markdown","id":"cef3c5d2-9c7c-4090-984c-486a0f523fc2","metadata":{},"outputs":[],"source":["As we are using the tensor <b>Y</b> for classification we cast it to a <code>LongTensor</code>. \n"]},{"cell_type":"code","id":"79dd865e-41fd-4c44-98dd-87b5f4f3f832","metadata":{},"outputs":[],"source":["Y=Y.type(torch.LongTensor)\nY.type()"]},{"cell_type":"markdown","id":"009f71f4-0414-49ca-9ab8-c29d5bf0e9c1","metadata":{},"outputs":[],"source":["With respect to each element we will set the even elements to class one and the odd elements to class zero.\n"]},{"cell_type":"code","id":"e99c8527-1005-4561-a70b-583ac43c93bd","metadata":{},"outputs":[],"source":["Y[::2]=1\nY[1::2]=0"]},{"cell_type":"markdown","id":"5a125d0e-b223-4344-acc5-b20a2d8c5d1e","metadata":{},"outputs":[],"source":["<h2 id=\"Question_2\">Question 2</h2>\n","<b>Create a list all_files such that the even indexes contain the path to images with positive or cracked samples and the odd element contain the negative images or images with out cracks. Then use the following code to print out the first four samples.</b>\n"]},{"cell_type":"code","id":"0018422e-723c-4dfd-9d20-59930f8ff009","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"3b2e1e10-6c7d-429e-b52a-bee8df1c35d2","metadata":{},"outputs":[],"source":["code used to print samples:\n"]},{"cell_type":"code","id":"352edbfe-9014-4e3a-a0b0-dd1e2637aae4","metadata":{},"outputs":[],"source":["for y,file in zip(Y, all_files[0:4]):\n    plt.imshow(Image.open(file))\n    plt.title(\"y=\"+str(y.item()))\n    plt.show()\n    "]},{"cell_type":"markdown","id":"27abbfa3-072c-42bc-a761-5cda964703fa","metadata":{},"outputs":[],"source":["<h2 id=\"split\">Training  and Validation  Split  </h2>\n","When training the model we  split up our data into training and validation data. It If the variable train is set to <code>True</code>  the following lines of code will segment the  tensor <b>Y</b> such at  the first 30000 samples are used for training. If the variable train is set to <code>False</code> the remainder of the samples will be used for validation data. \n"]},{"cell_type":"code","id":"ab1c0364-6c2e-4270-bbb5-fd8a91e8dc9a","metadata":{},"outputs":[],"source":["train=False\n\nif train:\n    all_files=all_files[0:30000]\n    Y=Y[0:30000]\n\nelse:\n    all_files=all_files[30000:]\n    Y=Y[30000:]"]},{"cell_type":"markdown","id":"08dc6f5f-4ebf-4ebd-ae8f-c4d0b115fec0","metadata":{},"outputs":[],"source":["<h2 id=\"Question_3\">Question 3</h2>\n","Modify the above lines of code such that if the variable <code>train</code> is set to <c>True</c> the first 30000 samples of all_files are use in training. If <code>train</code> is set to <code>False</code> the remaining  samples are used for validation. In both cases reassign  the values to the variable all_files, then use the following lines of code to print out the first four validation sample images.\n"]},{"cell_type":"code","id":"9d6da5c0-0374-4ea2-b725-f31f46742734","metadata":{},"outputs":[],"source":["\n    "]},{"cell_type":"markdown","id":"d0a12722-f73c-405d-af48-ece20ad93c59","metadata":{},"outputs":[],"source":["Just a note the images printed out in question two are the first four training samples.\n"]},{"cell_type":"markdown","id":"456a850d-7b7a-4cb3-804b-8bd8fb4b61ef","metadata":{},"outputs":[],"source":["<h2 id=\"data_class\">Create a Dataset Class</h2>\n"]},{"cell_type":"markdown","id":"e25ad484-a272-48c5-b39d-05eb69726393","metadata":{},"outputs":[],"source":["In this section, we will use the previous code to build a dataset class. \n"]},{"cell_type":"markdown","id":"4a120cff-e4b5-494f-a22e-cc1dc994b7ae","metadata":{},"outputs":[],"source":["\n","Complete the code to build a Dataset class <code>dataset</code>. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter <code>train</code> is set to <code>True</code>, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data.  \n"]},{"cell_type":"code","id":"08daec15-6d41-4365-99c2-4234beef1ae6","metadata":{},"outputs":[],"source":["class Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/resources/data\"\n        positive=\"Positive\"\n        negative=\"Negative\"\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n        positive_files.sort()\n        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n        negative_files.sort()\n\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)\n    \n  \n            \n     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n        \n        \n        image=Image.open(self.all_files[idx])\n        y=self.Y[idx]\n          \n        \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y"]},{"cell_type":"markdown","id":"cc7d343e-05a6-4eb1-b35a-7ba43716bd52","metadata":{},"outputs":[],"source":["<h2 id=\"Question_4\">Question 4</h2>\n","<b>Create a Dataset object <code>dataset</code> for the training data, use the following lines of code to print out sample the 10th and  sample 100 (remember zero indexing)  </b>\n"]},{"cell_type":"code","id":"ab1931be-3e55-4ff5-a0b4-a632461aaa4b","metadata":{},"outputs":[],"source":["\n\nfor sample  in samples:\n    plt.imshow(dataset[sample][0])\n    plt.xlabel(\"y=\"+str(dataset[sample][1].item()))\n    plt.title(\"training data, sample {}\".format(int(sample)))\n    plt.show()\n    "]},{"cell_type":"markdown","id":"094acb0b-2906-4084-9b23-f143a9778008","metadata":{},"outputs":[],"source":["We now have all the tools to create a list with the path to each image file.  We use a List Comprehensions  to make the code more compact. We assign it to the variable <code>negative_files<code> , sort it in and display the first three elements:\n"]},{"cell_type":"markdown","id":"dde4efea-19b8-4db1-bd06-3027781f3b1d","metadata":{},"outputs":[],"source":["<h2 id=\"Question_5\">Question 5</h2>\n","<b>Create a Dataset object <code>dataset</code> for the validation  data, use the following lines of code to print out the 16 th and  sample 103 (remember zero indexing)   </b>\n"]},{"cell_type":"code","id":"d8a1d019-03cf-4a72-9407-46f056c67e56","metadata":{},"outputs":[],"source":["\n\n\nfor sample  in samples:\n    plt.imshow(dataset[sample][0])\n    plt.xlabel(\"y=\"+str(dataset[sample][1].item()))\n    plt.title(\"validation data, sample {}\".format(int(sample)))\n    plt.show()"]},{"cell_type":"markdown","id":"caa49aa7-0d41-4f7a-a512-59fa3cb2be27","metadata":{},"outputs":[],"source":["<h2>About the Authors:</h2>\n"," <a href=\\\"https://www.linkedin.com/in/joseph-s-50398b136/\\\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"bc9349a0-ccca-471f-adf6-16533fa8f600","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n"]},{"cell_type":"markdown","id":"914d8264-75a7-44d6-9e40-25c3be11567c","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2fd091e0-4526-46e1-b14a-03411c293d69","metadata":{},"outputs":[],"source":["Copyright &copy; 2018 <a href=\"cognitiveclass.ai\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\\\"https://bigdatauniversity.com/mit-license/\\\">MIT License</a>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}