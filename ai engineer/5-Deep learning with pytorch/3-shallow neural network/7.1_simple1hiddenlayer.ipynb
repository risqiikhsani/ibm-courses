{"cells":[{"cell_type":"markdown","id":"7f8b8dce-8f43-416f-944f-69d770a62cf5","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"d20b7cf0-295c-49a2-bb9e-4e554e2ee87c","metadata":{},"outputs":[],"source":["<h1>Simple One Hidden Layer Neural Network</h1>\n"]},{"cell_type":"markdown","id":"d3185d47-d2bf-42cc-ba18-ede651b4dcae","metadata":{},"outputs":[],"source":["<h2>Objective</h2><ul><li> How to create simple Neural Network in pytorch.</li></ul> \n"]},{"cell_type":"markdown","id":"cba059c2-1efd-4f3b-926a-8b3317bc2eaa","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n","<p>In this lab, you will use a single-layer neural network to classify non linearly seprable data in 1-Ddatabase.</p>\n","\n","<ul>\n","    <li><a href=\"#Model\">Neural Network Module and Training Function</a></li>\n","    <li><a href=\"#Makeup_Data\">Make Some Data</a></li>\n","    <li><a href=\"#Train\">Define the Neural Network, Criterion Function, Optimizer, and Train the Model</a></li>\n","</ul>\n","<p>Estimated Time Needed: <strong>25 min</strong></p>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"20d5d3da-543d-4ae4-8386-a9daaea50db5","metadata":{},"outputs":[],"source":["<h2>Preparation</h2>\n"]},{"cell_type":"markdown","id":"1a59d38b-8457-4591-8650-b03f5841a00a","metadata":{},"outputs":[],"source":["We'll need the following libraries\n"]},{"cell_type":"code","id":"5487c618-038e-47a0-b4ea-52aea4c80840","metadata":{},"outputs":[],"source":["# Import the libraries we need for this lab\n\nimport torch \nimport torch.nn as nn\nfrom torch import sigmoid\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)"]},{"cell_type":"markdown","id":"79d3bfcd-c602-40e1-a3b9-9fab4239e3a8","metadata":{},"outputs":[],"source":["Used for plotting the model\n"]},{"cell_type":"code","id":"7123497a-ca9b-4ace-83c4-8199a169df36","metadata":{},"outputs":[],"source":["# The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    \n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass"]},{"cell_type":"markdown","id":"5ddb4d99-bf9c-4ad5-abc3-c21fe90a9013","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"d863ca87-d397-4fed-a5fb-c0daa30f671c","metadata":{},"outputs":[],"source":["<h2 id=\"Model\">Neural Network Module and Training Function</h2> \n"]},{"cell_type":"markdown","id":"fd3f4778-956e-43ca-ae1c-84afabfa83e0","metadata":{},"outputs":[],"source":["Define the activations and the output of the first linear layer as an attribute. Note that this is not good practice. \n"]},{"cell_type":"code","id":"cc0faeec-19ea-45a2-84d4-03fe1ec5c582","metadata":{},"outputs":[],"source":["# Define the class Net\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        # hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n        # Define the first linear layer as an attribute, this is not good practice\n        self.a1 = None\n        self.l1 = None\n        self.l2=None\n    \n    # Prediction\n    def forward(self, x):\n        self.l1 = self.linear1(x)\n        self.a1 = sigmoid(self.l1)\n        self.l2=self.linear2(self.a1)\n        yhat = sigmoid(self.linear2(self.a1))\n        return yhat"]},{"cell_type":"markdown","id":"1e2baabf-ca9b-4e70-a6b8-3ee0c9f5a801","metadata":{},"outputs":[],"source":["Define the training function:\n"]},{"cell_type":"code","id":"181d9171-3ab8-4fe9-80ac-a4f9368f3bae","metadata":{},"outputs":[],"source":["# Define the training function\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total=0\n    for epoch in range(epochs):\n        total=0\n        for y, x in zip(Y, X):\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            #cumulative loss \n            total+=loss.item() \n        cost.append(total)\n        if epoch % 300 == 0:    \n            PlotStuff(X, Y, model, epoch, leg=True)\n            plt.show()\n            model(X)\n            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\n            plt.title('activations')\n            plt.show()\n    return cost"]},{"cell_type":"markdown","id":"c9dc0083-da1a-474c-9edb-7fbb79df0171","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"9162e719-2160-4e77-b70f-510b50de731a","metadata":{},"outputs":[],"source":["<h2 id=\"Makeup_Data\">Make Some Data</h2>\n"]},{"cell_type":"code","id":"2ade1b64-95a5-476e-9862-9769a949441e","metadata":{},"outputs":[],"source":["# Make some data\n\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] > -4) & (X[:, 0] < 4)] = 1.0"]},{"cell_type":"markdown","id":"2221a3b7-e3c1-47b7-8b7d-f47f3b682668","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"dd271ba8-bbd4-4402-8584-7c173134f653","metadata":{},"outputs":[],"source":["<h2 id=\"Train\">Define the Neural Network, Criterion Function, Optimizer and Train the Model</h2>\n"]},{"cell_type":"markdown","id":"9fca0439-2247-422b-9da7-3fcd68b5510e","metadata":{},"outputs":[],"source":["Create the Cross-Entropy loss function: \n"]},{"cell_type":"code","id":"498f661c-8767-43df-aad5-e4b9567292ef","metadata":{},"outputs":[],"source":["# The loss function\n\ndef criterion_cross(outputs, labels):\n    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n    return out"]},{"cell_type":"markdown","id":"882b9ff6-0a39-448b-ab16-fd52caa41dbb","metadata":{},"outputs":[],"source":["Define the Neural Network, Optimizer, and Train the Model:\n"]},{"cell_type":"code","id":"80cf89ec-149a-44ca-b394-daf92ad30f2a","metadata":{},"outputs":[],"source":["# Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')"]},{"cell_type":"markdown","id":"12cc977d-0a6a-4149-b494-feac2ced186c","metadata":{},"outputs":[],"source":["By examining the output of the  activation, you see by the 600th epoch that the data has been mapped to a linearly separable space.\n"]},{"cell_type":"markdown","id":"8452e2ef-794e-487f-b886-66b52ba507d7","metadata":{},"outputs":[],"source":["we can make a prediction for a arbitrary one tensors \n"]},{"cell_type":"code","id":"52eb65f8-41a3-4db9-bb53-5713a807ce09","metadata":{},"outputs":[],"source":["x=torch.tensor([0.0])\nyhat=model(x)\nyhat"]},{"cell_type":"markdown","id":"a1297e95-47b6-429a-a0e0-97f8818ecd21","metadata":{},"outputs":[],"source":["we can make a prediction for some arbitrary one tensors  \n"]},{"cell_type":"code","id":"045f57a1-d000-4dee-b458-1f6ec44ea0c7","metadata":{},"outputs":[],"source":["X_=torch.tensor([[0.0],[2.0],[3.0]])\nYhat=model(X_)\nYhat"]},{"cell_type":"markdown","id":"dabc3137-a968-47cb-902b-0e766265f925","metadata":{},"outputs":[],"source":["we  can threshold the predication\n"]},{"cell_type":"code","id":"7f4b6dca-97ae-41d6-9c97-5695a004003f","metadata":{},"outputs":[],"source":["Yhat=Yhat>0.5\nYhat"]},{"cell_type":"markdown","id":"f4a1760b-dceb-46c1-8487-e6defcb7a223","metadata":{},"outputs":[],"source":["<h3>Practice</h3>\n"]},{"cell_type":"markdown","id":"0f6b208e-0712-459b-a0b5-9ca983931018","metadata":{},"outputs":[],"source":["Repeat the previous steps above by using the MSE cost or total loss: \n"]},{"cell_type":"code","id":"a64683f7-c31c-4ec6-a28e-bf159e55de5d","metadata":{},"outputs":[],"source":["# Practice: Train the model with MSE Loss Function\n\n# Type your code here"]},{"cell_type":"markdown","id":"7939a005-74a1-431d-9e71-084eb151f0a1","metadata":{},"outputs":[],"source":["Double-click <b>here</b> for the solution.\n","\n","<!-- \n","learning_rate = 0.1\n","criterion_mse=nn.MSELoss()\n","model=Net(D_in,H,D_out)\n","optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)\n","cost_mse=train(Y,X,model,optimizer,criterion_mse,epochs=1000)\n","plt.plot(cost_mse)\n","plt.xlabel('epoch')\n","plt.title('MSE loss ')\n","-->\n"]},{"cell_type":"markdown","id":"5caf9b0c-ccf8-4bae-858a-0abbda77fe6e","metadata":{},"outputs":[],"source":["\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","id":"0941b79c-144d-4c2f-a673-334683093399","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"62b43773-422d-41be-a0d7-1fdb42ed4542","metadata":{},"outputs":[],"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD. \n"]},{"cell_type":"markdown","id":"f42a2f96-dc02-46f8-8a4d-38340d3e5148","metadata":{},"outputs":[],"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>\n"]},{"cell_type":"markdown","id":"8d015110-1eb5-4676-9fa0-c9dba25b035c","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","-->\n"]},{"cell_type":"markdown","id":"86b5dd0f-1d8f-4e7f-ac47-b4e46a94ec2f","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"089dc3e8-6478-4c06-8cfc-37d68e274df8","metadata":{},"outputs":[],"source":["\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}